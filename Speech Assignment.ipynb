{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave, os, glob, csv, sys, pathlib, math, gc\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy import save, load, savez\n",
    "from python_speech_features import mfcc\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "# import cv2\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "dir_path = os.path.abspath('')\n",
    "os.chdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shaun\\\\Documents\\\\MSc AI\\\\Semester 2\\\\ANLP\\\\Speech\\\\Assignment'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at the files in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain list of dataset, labels and speakers\n",
    "this_directory = os.getcwd()\n",
    "filelist = []\n",
    "filepathlist = []\n",
    "ids = []\n",
    "\n",
    "def get_file_paths(dirname):\n",
    "    file_paths = []\n",
    "    subdir_paths = []\n",
    "    for root, directories, files in os.walk(dirname):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)\n",
    "    return file_paths    \n",
    "\n",
    "def get_dataset_list():\n",
    "    files = get_file_paths(this_directory)                 \n",
    "    for file in files:                              \n",
    "        (filepath, ext) = os.path.splitext(file)    \n",
    "        file_name = os.path.basename(file)\n",
    "        sub_directory = os.path.dirname(file)\n",
    "        sub_directory = sub_directory.split('/')[-1]\n",
    "        if ext == '.wav':                           \n",
    "            filelist.append(file_name)\n",
    "            filepathlist.append(sub_directory)\n",
    "    for file in filelist:\n",
    "        ids.append(file[0:7])\n",
    "    dataset_list = list(zip(filelist, filepathlist, ids))\n",
    "    return dataset_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_list = get_dataset_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at training + test + validation sizes\n",
    "training_size = round(0.7*len(dataset_list))\n",
    "validation_size = round(0.1*len(dataset_list))\n",
    "test_size = len(dataset_list) - (training_size + validation_size)\n",
    "\n",
    "#Sanity Check\n",
    "print(training_size + validation_size + test_size)\n",
    "print(len(dataset_list))\n",
    "\n",
    "#Convert to dataframe, create labels list\n",
    "dataset_df = pd.DataFrame(dataset_list, columns = (\"FileName\", \"Label\", \"Speaker\"))\n",
    "labels = set(filepathlist)\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Dataset for Imbalance Check\n",
    "f, ax = plt.subplots(figsize=(15,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.countplot(x=\"Label\",\n",
    "data=dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at unusable recordings shorter than 1 second and removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_of_recordings=[]\n",
    "unusable_files = []\n",
    "waves = [file for file in dataset_df[\"FileName\"]]\n",
    "for index, row in dataset_df.iterrows():\n",
    "    sample_rate, samples = wavfile.read(os.getcwd()+ \"/\" + row['Label'] + \"/\" + row['FileName'])\n",
    "    duration_of_recordings.append(float(len(samples)/sample_rate))\n",
    "    if samples.shape[0] < sample_rate:\n",
    "        unusable_files.append(row['Label'] + \"/\" + row['FileName'])\n",
    "        dataset_df.drop(index, inplace=True)\n",
    "    gc.collect\n",
    "\n",
    "plt.hist(np.array(duration_of_recordings), range=(0.6,1.3))\n",
    "plt.xlabel('Duration in seconds')\n",
    "plt.ylabel('Quantity of files')\n",
    "plt.savefig('duration.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Dataset for Imbalance Check after dropping unusable files\n",
    "f, ax = plt.subplots(figsize=(15,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "imb = sns.countplot(x=\"Label\",\n",
    "                    data=dataset_df,\n",
    "                    palette=\"Greens_d\")\n",
    "plt.savefig('dataimb.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into separate dataframes\n",
    "grouped_df = dataset_df.groupby(dataset_df.Label)\n",
    "\n",
    "house_df = grouped_df.get_group(\"house\")\n",
    "cat_df = grouped_df.get_group(\"cat\")\n",
    "bird_df = grouped_df.get_group(\"bird\")\n",
    "left_df = grouped_df.get_group(\"left\")\n",
    "off_df = grouped_df.get_group(\"off\")\n",
    "dog_df = grouped_df.get_group(\"dog\")\n",
    "marvin_df = grouped_df.get_group(\"marvin\")\n",
    "backward_df = grouped_df.get_group(\"backward\")\n",
    "go_df = grouped_df.get_group(\"go\")\n",
    "visual_df = grouped_df.get_group(\"visual\")\n",
    "\n",
    "df_sublist = []\n",
    "df_train_sublist = []\n",
    "df_testval_sublist = []\n",
    "for label in labels:\n",
    "    df_sublist.append((\"{}_df\").format(label))\n",
    "    df_train_sublist.append((\"{}_df_train\").format(label))\n",
    "    df_testval_sublist.append((\"{}_df_testval\").format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sublist)\n",
    "print(df_train_sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_to_training_set(df):\n",
    "    if len(df[df.duplicated(\"Speaker\", keep=False)]) <= (0.7* len(df)):\n",
    "        print(\"{:.2%}\".format(len(df[df.duplicated(\"Speaker\", keep=False)])/len(df)))\n",
    "    else: \n",
    "        print(\"{:.2%}\".format(len(df[df.duplicated(\"Speaker\", keep=False)])/len(df)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing files for training set and checking the 70% split for data balance\n",
    "house_df_train =check_duplicates_to_training_set(house_df)\n",
    "cat_df_train =check_duplicates_to_training_set(cat_df)\n",
    "bird_df_train =check_duplicates_to_training_set(bird_df)\n",
    "left_df_train =check_duplicates_to_training_set(left_df)\n",
    "off_df_train =check_duplicates_to_training_set(off_df)\n",
    "dog_df_train =check_duplicates_to_training_set(dog_df)\n",
    "marvin_df_train =check_duplicates_to_training_set(marvin_df)\n",
    "backward_df_train = check_duplicates_to_training_set(backward_df)\n",
    "go_df_train =check_duplicates_to_training_set(go_df)\n",
    "visual_df_train =check_duplicates_to_training_set(visual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many recordings have others by the same speaker\n",
    "num_of_dupes = 0\n",
    "for df in df_sublist:\n",
    "    print(len(eval(df)[eval(df).duplicated(\"Speaker\", keep=False)]))\n",
    "    num_of_dupes += (len(eval(df)[eval(df).duplicated(\"Speaker\", keep=False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.2%}\".format(num_of_dupes/len(dataset_list)))\n",
    "#Since this is below 70% we could proceed to add all the data to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_duplicates_to_training_set(df):\n",
    "    return df[df.duplicated(\"Speaker\", keep=False)], df.drop_duplicates(subset=\"Speaker\", keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populating training and test/val dataframes\n",
    "house_df_train, house_df_testval = add_duplicates_to_training_set(house_df)\n",
    "cat_df_train,cat_df_testval = add_duplicates_to_training_set(cat_df)\n",
    "bird_df_train, bird_df_testval = add_duplicates_to_training_set(bird_df)\n",
    "left_df_train, left_df_testval = add_duplicates_to_training_set(left_df)\n",
    "off_df_train, off_df_testval = add_duplicates_to_training_set(off_df)\n",
    "dog_df_train, dog_df_testval = add_duplicates_to_training_set(dog_df)\n",
    "marvin_df_train, marvin_df_testval = add_duplicates_to_training_set(marvin_df)\n",
    "backward_df_train, backward_df_testval = add_duplicates_to_training_set(backward_df)\n",
    "go_df_train, go_df_testval = add_duplicates_to_training_set(go_df)\n",
    "visual_df_train, visual_df_testval = add_duplicates_to_training_set(visual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack test_validation dataframe\n",
    "test_val_array = np.vstack(eval(df).values for df in df_testval_sublist)\n",
    "test_val_array = test_val_array[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Test and Validation Sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(test_val_array[:,:-1], test_val_array[:,-1], test_size = 0.67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating test and validation sets\n",
    "val_set = np.column_stack((X_val,y_val))\n",
    "test_set = np.column_stack((X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_speech_mfcc(raw_train_data, label):\n",
    "    label_vector = []\n",
    "    dirname = os.getcwd()\n",
    "    for index, row in raw_train_data.iterrows():\n",
    "        (rate,sig) = wavfile.read(dirname + \"/\" + row['Label'] + \"/\" + row['FileName'])\n",
    "        mfcc_feat = mfcc(sig,rate,nfft=1024)\n",
    "        label_vector.append(mfcc_feat)\n",
    "        gc.collect\n",
    "    label_array = np.concatenate(label_vector, axis=0)\n",
    "    b = [label for i in range(label_array.shape[0])]\n",
    "    label_array = np.column_stack((label_array, b))\n",
    "#     print(raw_train_data.head(),label)\n",
    "#     print(label_array.shape)\n",
    "    return label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test_Val Set MFCC\n",
    "mfcc_df_testval = np.empty((0,14))\n",
    "\n",
    "for df, label in zip(df_testval_sublist, labels):\n",
    "    mfcc_df_testval = np.append(mfcc_df_testval, python_speech_mfcc(eval(df), label), axis=0)\n",
    "    gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Set MFCC\n",
    "mfcc_df_train = np.empty((0,14))\n",
    "\n",
    "for df, label in zip(df_train_sublist, labels):\n",
    "    mfcc_df_train = np.append(mfcc_df_train, python_speech_mfcc(eval(df), label), axis=0)\n",
    "    gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to numpy array files\n",
    "save('mfcc_testval.npy', mfcc_df_testval)\n",
    "save('mfcc_train.npy', mfcc_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to zipped numpy array file\n",
    "mfcc_df_testval = load('mfcc_testval_clean.npz')\n",
    "mfcc_df_train = load('mfcc_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Test and Val Sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(mfcc_df_testval['arr_0'][:,:-1], mfcc_df_testval['arr_0'][:,-1], test_size = 0.67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting X-train and y-train\n",
    "X_train = mfcc_df_train[:, :-1]\n",
    "y_train = mfcc_df_train[:, -1]\n",
    "\n",
    "# Shuffling the training set\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 4 1 ... 8 5 2]\n",
      "[7 3 1 ... 2 1 8]\n",
      "[9 3 5 ... 6 1 7]\n"
     ]
    }
   ],
   "source": [
    "# #Encoding the labels\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "lenc = LabelEncoder()\n",
    "y_train = lenc.fit_transform(y_train)\n",
    "y_val = lenc.fit_transform(y_val)\n",
    "y_test = lenc.fit_transform(y_test)\n",
    "print(y_train)\n",
    "print(y_val)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # One Hot Encoding cause multiclass classification\n",
    "# from keras.utils import np_utils\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1530639, 10)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# savez('mfcc_testval_clean', mfcc_df_testval)\n",
    "# savez('mfcc_train_clean', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_val = X_val.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.str_'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Check Types to ensure sparse/dense matrices\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))\n",
    "print(type(X_val))\n",
    "print(type(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - GMM-MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Due to very large dimensionality of the matrix and problems with local system memory handling the array, we apply PCA and attempt to fit to a reduced matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-263a4ad79368>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m750\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m models = [GMM(n, covariance_type='spherical', random_state=0).fit(X_train)\n\u001b[1;32m---> 12\u001b[1;33m           for n in n_components]\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# fig, gmm = plt.subplots(figsize=(10, 6))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-263a4ad79368>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m750\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m models = [GMM(n, covariance_type='spherical', random_state=0).fit(X_train)\n\u001b[1;32m---> 12\u001b[1;33m           for n in n_components]\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# fig, gmm = plt.subplots(figsize=(10, 6))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\sklearn\\mixture\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \"\"\"\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\sklearn\\mixture\\_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdo_init\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mlower_bound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfty\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdo_init\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower_bound_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\sklearn\\mixture\\_base.py\u001b[0m in \u001b[0;36m_initialize_parameters\u001b[1;34m(self, X, random_state)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n\u001b[1;32m--> 147\u001b[1;33m                                    random_state=random_state).fit(X).labels_\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_params\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'random'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m                     \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m                     x_squared_norms=x_squared_norms, random_state=seed)\n\u001b[0m\u001b[0;32m    938\u001b[0m                 \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;31m# init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n\u001b[1;32m--> 314\u001b[1;33m                               x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[0mcenters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_init_centroids\u001b[1;34m(X, k, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         centers = _k_init(X, k, random_state=random_state,\n\u001b[1;32m--> 626\u001b[1;33m                           x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[0;32m    627\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'random'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_k_init\u001b[1;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    119\u001b[0m         np.minimum(closest_dist_sq, distance_to_candidates,\n\u001b[0;32m    120\u001b[0m                    out=distance_to_candidates)\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mcandidates_pot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_to_candidates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Decide which candidate is the best\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(0.85, whiten=True)\n",
    "# pca.fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "# X_val = pca.transform(X_val)\n",
    "\n",
    "n_components = np.arange(750, 2250, 750)\n",
    "models = [GMM(n, covariance_type='spherical', random_state=0).fit(X_train)\n",
    "          for n in n_components]\n",
    "\n",
    "# fig, gmm = plt.subplots(figsize=(10, 6))\n",
    "# bics = [ model.fit(X_train).bic(X_train) for model in models ]\n",
    "# aics = [ model.fit(X_train).aic(X_train) for model in models ]\n",
    "# plt.plot(n_components, bics, label='AIC')\n",
    "# gmm.legend(loc='best')\n",
    "# gmm.set_xlabel('n_components')\n",
    "# gmm.set_xticks(np.arange(0, 120, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GMM Bayes\n",
    "---------\n",
    "This implements generative classification based on mixtures of gaussians\n",
    "to model the probability density of each class.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import BaseNB\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "class GMMBayes(BaseNB):\n",
    "    \"\"\"GaussianMixture Bayes Classifier\n",
    "\n",
    "    This is a generalization to the Naive Bayes classifier: rather than\n",
    "    modeling the distribution of each class with axis-aligned gaussians,\n",
    "    GMMBayes models the distribution of each class with mixtures of\n",
    "    gaussians.  This can lead to better classification in some cases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int or list\n",
    "        number of components to use in the GaussianMixture. If specified as\n",
    "        a list, it must match the number of class labels. Default is 1.\n",
    "    **kwargs : dict, optional\n",
    "        other keywords are passed directly to GaussianMixture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=1, **kwargs):\n",
    "        self.n_components = np.atleast_1d(n_components)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if n_samples != y.shape[0]:\n",
    "            raise ValueError(\"X and y have incompatible shapes\")\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.classes_.sort()\n",
    "        unique_y = self.classes_\n",
    "\n",
    "        n_classes = unique_y.shape[0]\n",
    "\n",
    "        if self.n_components.size not in (1, len(unique_y)):\n",
    "            raise ValueError(\"n_components must be compatible with \"\n",
    "                             \"the number of classes\")\n",
    "\n",
    "        self.gmms_ = [None for i in range(n_classes)]\n",
    "        self.class_prior_ = np.zeros(n_classes)\n",
    "\n",
    "        n_comp = np.zeros(len(self.classes_), dtype=int) + self.n_components\n",
    "        gc.collect()\n",
    "        for i, y_i in enumerate(unique_y):\n",
    "            if n_comp[i] > X[y == y_i].shape[0]:\n",
    "                warnstr = (\"Expected n_samples >= n_components but got \"\n",
    "                           \"n_samples={0}, n_components={1}, \"\n",
    "                           \"n_components set to {0}.\")\n",
    "                warnings.warn(warnstr.format(X[y == y_i].shape[0], n_comp[i]))\n",
    "                n_comp[i] = y_i\n",
    "            self.gmms_[i] = GaussianMixture(n_comp[i], **self.kwargs).fit(X[y == y_i])\n",
    "            self.class_prior_[i] = np.float(np.sum(y == y_i)) / n_samples\n",
    "            gc.collect()\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \n",
    "        X = np.asarray(np.atleast_2d(X))\n",
    "        logprobs = np.array([g.score_samples(X) for g in self.gmms_]).T\n",
    "        return logprobs + np.log(self.class_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness: 0.404752\n",
      "Contamination: 0.586042\n"
     ]
    }
   ],
   "source": [
    "# Fit the GMMNaive Bayes classifier to the reduced dimensions\n",
    "gmm_nb = GMMBayes(100) # 250 components per class\n",
    "gmm_nb.fit(X_train, y_train)\n",
    "gc.collect()\n",
    "# now predict\n",
    "y_pred = gmm_nb.predict(X_train)\n",
    "\n",
    "#get completeness score (equivalent to recall)\n",
    "completeness_score = recall_score(y_train,y_pred, average='weighted')\n",
    "#get contamination score (equivalent to 1-precision)\n",
    "contamination_score = (1-precision_score(y_train,y_pred, average='weighted'))\n",
    "\n",
    "print('Completeness: %f'%completeness_score)\n",
    "print('Contamination: %f'%contamination_score)\n",
    "filename = 'gmm_model_100.sav'\n",
    "pickle.dump(gmm_nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMMNaive Bayes classifier to the reduced dimensions\n",
    "gmm_nb = GMMBayes(500) # 500 components per class\n",
    "gmm_nb.fit(X_train, y_train)\n",
    "gc.collect()\n",
    "# now predict\n",
    "y_pred = gmm_nb.predict(X_train)\n",
    "\n",
    "#get completeness score (equivalent to recall)\n",
    "completeness_score = recall_score(y_train,y_pred, average='weighted')\n",
    "#get contamination score (equivalent to 1-precision)\n",
    "contamination_score = (1-precision_score(y_train,y_pred, average='weighted'))\n",
    "\n",
    "print('Completeness: %f'%completeness_score)\n",
    "print('Contamination: %f'%contamination_score)\n",
    "filename = 'gmm_model_10_06.sav'\n",
    "pickle.dump(gmm_nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMMNaive Bayes classifier to the reduced dimensions\n",
    "gmm_nb = GMMBayes(750) # 750 components per class\n",
    "gmm_nb.fit(X_train, y_train)\n",
    "gc.collect()\n",
    "# now predict\n",
    "y_pred = gmm_nb.predict(X_train)\n",
    "\n",
    "#get completeness score (equivalent to recall)\n",
    "completeness_score = recall_score(y_train,y_pred, average='weighted')\n",
    "#get contamination score (equivalent to 1-precision)\n",
    "contamination_score = (1-precision_score(y_train,y_pred, average='weighted'))\n",
    "\n",
    "print('Completeness: %f'%completeness_score)\n",
    "print('Contamination: %f'%contamination_score)\n",
    "filename = 'gmm_model_10_06_750.sav'\n",
    "pickle.dump(gmm_nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMMNaive Bayes classifier to the reduced dimensions\n",
    "gmm_nb = GMMBayes(1000) # 1000 components per class\n",
    "gmm_nb.fit(X_train, y_train)\n",
    "gc.collect()\n",
    "# now predict\n",
    "y_pred = gmm_nb.predict(X_train)\n",
    "\n",
    "#get completeness score (equivalent to recall)\n",
    "completeness_score = recall_score(y_train,y_pred, average='weighted')\n",
    "#get contamination score (equivalent to 1-precision)\n",
    "contamination_score = (1-precision_score(y_train,y_pred, average='weighted'))\n",
    "\n",
    "print('Completeness: %f'%completeness_score)\n",
    "print('Contamination: %f'%contamination_score)\n",
    "filename = 'gmm_model_13_06_1000.sav'\n",
    "pickle.dump(gmm_nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMMNaive Bayes classifier to the reduced dimensions\n",
    "gmm_nb = GMMBayes(1500) # 2000 components per class\n",
    "gmm_nb.fit(X_train, y_train)\n",
    "gc.collect()\n",
    "# now predict\n",
    "y_pred = gmm_nb.predict(X_train)\n",
    "\n",
    "#get completeness score (equivalent to recall)\n",
    "completeness_score = recall_score(y_train,y_pred, average='weighted')\n",
    "#get contamination score (equivalent to 1-precision)\n",
    "contamination_score = (1-precision_score(y_train,y_pred, average='weighted'))\n",
    "\n",
    "print('Completeness: %f'%completeness_score)\n",
    "print('Contamination: %f'%contamination_score)\n",
    "filename = 'gmm_model_13_06_1500.sav'\n",
    "pickle.dump(gmm_nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "def model_load_test(filename, X_train, y_train, X_test, y_test):\n",
    "    gmm_nb = pickle.load(open(filename, 'rb'))\n",
    "    y_pred = gmm_nb.predict(X_train)\n",
    "\n",
    "    #get completeness score (equivalent to recall)\n",
    "    completeness_score = recall_score(y_train,y_pred, average='weighted')\n",
    "    #get contamination score (equivalent to 1-precision)\n",
    "    contamination_score = (1-precision_score(y_train,y_pred, average='weighted'))\n",
    "    print('Completeness: %f'%completeness_score)\n",
    "    print('Contamination: %f'%contamination_score)\n",
    "    \n",
    "    y_pred = gmm_nb.predict(X_test)\n",
    "    completeness_score = recall_score(y_test,y_pred, average='weighted')\n",
    "    contamination_score = (1-precision_score(y_test,y_pred, average='weighted'))\n",
    "    print('Completeness Test: %f'%completeness_score)\n",
    "    print('Contamination Test: %f'%contamination_score)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness: 0.625688\n",
      "Contamination: 0.370007\n",
      "Completeness Test: 0.145033\n",
      "Contamination Test: 0.779783\n"
     ]
    }
   ],
   "source": [
    "model_load_test('gmm_model_13_06_1500.sav', X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-268-2de8a5bba4f4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-268-2de8a5bba4f4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NEED TO TEST HERE\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating spectrogram files for every training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_spectrogram(raw_train_data, label):\n",
    "    dirname = os.getcwd()\n",
    "    pathlib.Path(dirname + \"/\" + label + '/images').mkdir(parents=True, exist_ok=True)\n",
    "    fig,ax = plt.subplots(1)\n",
    "    for index, row in raw_train_data.iterrows():\n",
    "        samplingFrequency, signalData = wavfile.read(dirname + \"/\" + row['Label'] + \"/\" + row['FileName'])\n",
    "        fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "        ax.axis('off')\n",
    "        pxx, freqs, bins, im = ax.specgram(x=signalData, Fs=samplingFrequency, noverlap=240, NFFT=512, cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        fig.savefig(dirname + \"/\" + label + '/images/' + row[\"FileName\"] +'.png', dpi=300, frameon='false', transparent=True)\n",
    "        plt.cla()\n",
    "    plt.close(fig)\n",
    "        \n",
    "for df, label in zip(df_train_sublist, labels):\n",
    "    graph_spectrogram(eval(df), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the same to test array\n",
    "dirname = os.getcwd()\n",
    "for label in labels:\n",
    "    pathlib.Path(dirname + \"/\" + 'test/' + label).mkdir(parents=True, exist_ok=True)\n",
    "fig,ax = plt.subplots(1)\n",
    "for row in test_set:\n",
    "    samplingFrequency, signalData = wavfile.read(dirname + \"/\" + row[1] + \"/\" + row[0])\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "    ax.axis('off')\n",
    "    pxx, freqs, bins, im = ax.specgram(x=signalData, Fs=samplingFrequency, noverlap=240, NFFT=512, cmap='viridis')\n",
    "    ax.axis('off')\n",
    "    fig.savefig(dirname + \"/\" + 'test/' + row[1] + '/'+ row[0] +'.png', dpi=300, frameon='false', transparent=True)\n",
    "    plt.cla()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the same to val array\n",
    "dirname = os.getcwd()\n",
    "for label in labels:\n",
    "    pathlib.Path(dirname + \"/\" + 'val/' + label).mkdir(parents=True, exist_ok=True)\n",
    "fig,ax = plt.subplots(1)\n",
    "for row in val_set:\n",
    "    samplingFrequency, signalData = wavfile.read(dirname + \"/\" + row[1] + \"/\" + row[0])\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "    ax.axis('off')\n",
    "    pxx, freqs, bins, im = ax.specgram(x=signalData, Fs=samplingFrequency, noverlap=240, NFFT=512, cmap='viridis')\n",
    "    ax.axis('off')\n",
    "    fig.savefig(dirname + \"/\" + 'val/' + row[1] + '/'+ row[0] +'.png', dpi=300, frameon='false', transparent=True)\n",
    "    plt.cla()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the first instance, the data was passed to the CNN by means of the ImageDataGenerator function. In order to make use of better processing power through Google Colab and hence perform more experiments, this was then replaced by the Colab variant explained further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15461 images belonging to 10 classes.\n",
      "Found 2286 images belonging to 10 classes.\n",
      "Found 4649 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescale all pixel values from 0-255, so aftre this step all our pixel values are in range (0,1)\n",
    "        shear_range=0.2, #to apply some random tranfromations\n",
    "        zoom_range=0.2,#to apply zoom\n",
    "        horizontal_flip=True) # image will be flipper horiz\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "dirname = os.getcwd()\n",
    "\n",
    "img_height, img_width = (64,64)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        dirname,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='categorical',\n",
    "        classes = list(labels),\n",
    "        shuffle = True)\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory(\n",
    "        dirname+'/val/',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='categorical',\n",
    "        classes = list(labels))\n",
    "\n",
    "testing_set = test_datagen.flow_from_directory(\n",
    "        dirname+'/test/',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='categorical',\n",
    "        shuffle = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\shaun\\anaconda3\\envs\\speech\\lib\\site-packages (20.1.1)\n",
      "Requirement already up-to-date: setuptools in c:\\users\\shaun\\appdata\\roaming\\python\\python36\\site-packages (47.3.1)\n",
      "Requirement already up-to-date: wheel in c:\\users\\shaun\\appdata\\roaming\\python\\python36\\site-packages (0.34.2)\n",
      "Collecting tensorflow"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts easy_install-3.6.exe and easy_install.exe are installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script chardetect.exe is installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script wheel.exe is installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached tensorflow-2.2.0-cp36-cp36m-win_amd64.whl (459.1 MB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Using cached protobuf-3.12.2-cp36-cp36m-win_amd64.whl (1.1 MB)\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Processing c:\\users\\shaun\\appdata\\local\\pip\\cache\\wheels\\93\\2a\\eb\\e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.29.0-cp36-cp36m-win_amd64.whl (2.4 MB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp36-cp36m-win_amd64.whl (2.4 MB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Using cached wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-win_amd64.whl (30.8 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting numpy<2.0,>=1.16.0\n",
      "  Using cached numpy-1.19.0-cp36-cp36m-win_amd64.whl (13.0 MB)\n",
      "Processing c:\\users\\shaun\\appdata\\local\\pip\\cache\\wheels\\32\\42\\7f\\23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\\wrapt-1.12.1-py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Processing c:\\users\\shaun\\appdata\\local\\pip\\cache\\wheels\\c3\\af\\84\\3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\\absl_py-0.9.0-py3-none-any.whl\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Using cached setuptools-47.3.1-py3-none-any.whl (582 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Using cached importlib_metadata-1.6.1-py2.py3-none-any.whl (31 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: six, absl-py, cachetools, setuptools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, idna, urllib3, chardet, certifi, requests, requests-oauthlib, google-auth-oauthlib, grpcio, werkzeug, zipp, importlib-metadata, markdown, numpy, tensorboard-plugin-wit, wheel, protobuf, tensorboard, google-pasta, termcolor, h5py, tensorflow-estimator, scipy, gast, astunparse, wrapt, keras-preprocessing, opt-einsum, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.0 certifi-2020.6.20 chardet-3.0.4 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.29.0 h5py-2.10.0 idna-2.9 importlib-metadata-1.6.1 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.19.0 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.5.0 setuptools-47.3.1 six-1.15.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 urllib3-1.25.9 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1 zipp-3.1.0\n",
      "Collecting keras\n",
      "  Using cached Keras-2.4.2-py2.py3-none-any.whl (170 kB)\n",
      "Collecting numpy>=1.9.1\n",
      "  Using cached numpy-1.19.0-cp36-cp36m-win_amd64.whl (13.0 MB)\n",
      "Collecting h5py\n",
      "  Using cached h5py-2.10.0-cp36-cp36m-win_amd64.whl (2.4 MB)\n",
      "Collecting scipy>=0.14\n",
      "  Using cached scipy-1.5.0-cp36-cp36m-win_amd64.whl (31.2 MB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.3.1-cp36-cp36m-win_amd64.whl (215 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: numpy, six, h5py, scipy, pyyaml, keras\n",
      "Successfully installed h5py-2.10.0 keras-2.4.2 numpy-1.19.0 pyyaml-5.3.1 scipy-1.5.0 six-1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.5.0 which is incompatible.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\Shaun\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel --user\n",
    "!pip install -I tensorflow --user\n",
    "!pip install -I keras --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'name_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-f0e095bd4f8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mget_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m# learning_phase_scope = tf_keras_backend.learning_phase_scope  # TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mname_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'name_scope'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-1686eb05d2d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;31m# We still need all the names that are toplevel on tensorflow_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_core\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;31m# These should not be visible in the main tf module.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautodiff\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v2\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mragged\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mraw_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[1;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Define CNN Model\n",
    "model = Sequential()\n",
    "input_shape = (img_height, img_width, 1)\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "nb_epochs = 20\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#Compile Model\n",
    "model.compile(optimizers.rmsprop(lr=0.0005, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit_generator(\n",
    "    training_set,\n",
    "    steps_per_epoch = training_set.samples // batch_size,\n",
    "    validation_data = validation_set, \n",
    "    validation_steps = validation_set.samples // batch_size,\n",
    "    epochs = nb_epochs)\n",
    "\n",
    "our_model = model\n",
    "our_model.save('dirname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-557703b66ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to attempt more experiments using Google Colab, mounting the spectrogram image files and passing them through the imagedatagenerator was proving to be too slow, so we opted to transform dataset into h5 files, reading the images as numpy arrays, and passing these to the notebook on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_write_data_into_h5_file(dest_filepath, filepaths_list, n_px, n_channels):\n",
    "  \n",
    "    data_shape = (len(filepaths_list), n_px * n_px * n_channels)\n",
    "    dataset_name = \"input_data\"\n",
    "    \n",
    "    with h5py.File(dest_filepath, 'a') as f:\n",
    "        \n",
    "        f.create_dataset(dataset_name, data_shape, np.float32)\n",
    "        \n",
    "        for i in range(len(filepaths_list)):\n",
    "            #if (i+1) % 512 == 0:\n",
    "            #    print('{}/{} files converted'.format((i+1), len(filepaths_list)))\n",
    "\n",
    "            filepath = filepaths_list[i]\n",
    "            img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (n_px, n_px), interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            #Normalize the image - convert the each pixel value between 0 and 1\n",
    "            img = img / 255\n",
    "            #Reshape the image - roll it up into a column vector\n",
    "            img = img.ravel()\n",
    "            \n",
    "            #img[None] makes it a proper array instead of rank 1 array\n",
    "            f[dataset_name][i, ...] = img[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_into_h5_file(dest_filepath, new_labels):\n",
    "    \n",
    "    dataset_name = \"input_labels\"\n",
    "    \n",
    "    with h5py.File(dest_filepath, 'a') as f:\n",
    "        f.create_dataset(dataset_name, (len(new_labels),), np.int8)\n",
    "        f[dataset_name][...] = new_labels\n",
    "        \n",
    "def convert_images_to_data_in_h5_file(src_img_filepath_pattern, dest_h5_file_path, n_px, \n",
    "                                      n_channels = 3, batch_size = 1024):\n",
    "    full_filep = np.empty((0,2))\n",
    "    for label in labels:\n",
    "        temp_filepaths = glob.glob(dirname+'/test/'+label+'/*.png') #Use this for test set\n",
    "#         temp_filepaths = glob.glob(dirname+'/val/'+label+'/*.png') #Use this for validation set\n",
    "#         temp_filepaths = glob.glob(dirname+ '/' + label+'/images/*.png') #Use this for training set\n",
    "        label_list= [label] * len(temp_filepaths)\n",
    "        temp_array = np.column_stack((temp_filepaths, label_list))\n",
    "        full_filep = np.append(full_filep, temp_array, axis=0)\n",
    "\n",
    "    np.random.shuffle(full_filep)\n",
    "#     print(full_filep)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    full_filep[:,1] = le.fit_transform(full_filep[:,1])\n",
    "    full_filepaths = list(full_filep[:,0])\n",
    "    new_labels = list(full_filep[:,1].astype(int))\n",
    "    t = list(zip(full_filepaths, new_labels))\n",
    "    shuffle(t)\n",
    "    #Get the shuffled filepaths & labels\n",
    "    src_filepaths, new_labels = zip(*t)\n",
    "\n",
    "#     print(full_filepaths)\n",
    "    \n",
    "    #Number of images\n",
    "    m = len(src_filepaths)\n",
    "#     print(m)\n",
    "    n_complete_batches = math.ceil(m / batch_size)\n",
    "#     print(n_complete_batches)\n",
    "    \n",
    "    for i in range(n_complete_batches):\n",
    "        print('Creating file', (i+1))\n",
    "        \n",
    "        dest_file_path = dest_h5_file_path + str(i + 1) + \".h5\"   \n",
    "        \n",
    "        start_pos = i * batch_size\n",
    "        end_pos = min(start_pos + batch_size, m)\n",
    "        src_filepaths_batch = src_filepaths[start_pos: end_pos]\n",
    "        labels_batch = new_labels[start_pos: end_pos]\n",
    "#         print(labels_batch)\n",
    "        normalize_and_write_data_into_h5_file(dest_file_path, src_filepaths_batch, n_px, n_channels)\n",
    "        write_labels_into_h5_file(dest_file_path, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file 1\n",
      "(8, 4, 1, 7, 6, 1, 9, 8, 6, 5, 3, 7, 6, 2, 1, 4, 5, 6, 3, 2, 5, 3, 1, 3, 3, 3, 9, 7, 2, 7, 7, 6, 4, 3, 2, 2, 2, 1, 8, 7, 8, 6, 5, 7, 2, 5, 3, 3, 3, 5, 6, 1, 4, 7, 6, 5, 7, 5, 8, 1, 6, 3, 3, 2, 7, 3, 4, 4, 3, 5, 7, 4, 2, 5, 2, 1, 8, 5, 8, 5, 5, 3, 3, 3, 5, 5, 6, 3, 8, 7, 2, 7, 6, 3, 4, 8, 2, 5, 4, 5, 7, 7, 4, 9, 3, 8, 5, 1, 8, 4, 5, 3, 1, 2, 7, 7, 3, 4, 2, 7, 3, 9, 4, 3, 7, 7, 7, 5, 2, 5, 2, 3, 3, 5, 5, 6, 1, 1, 6, 3, 3, 3, 4, 7, 1, 5, 2, 5, 3, 1, 1, 3, 2, 7, 1, 1, 4, 6, 5, 3, 6, 7, 2, 2, 2, 4, 2, 3, 3, 5, 8, 3, 5, 6, 6, 1, 1, 9, 7, 1, 4, 7, 2, 3, 8, 3, 8, 1, 5, 6, 3, 4, 3, 2, 2, 7, 3, 7, 6, 1, 3, 4, 4, 5, 4, 5, 5, 1, 2, 3, 7, 8, 6, 5, 1, 8, 7, 6, 7, 1, 2, 2, 6, 8, 7, 5, 5, 1, 8, 2, 7, 9, 8, 2, 3, 1, 8, 5, 2, 5, 1, 6, 5, 3, 3, 1, 5, 3, 5, 6, 2, 8, 6, 4, 5, 6, 3, 1, 4, 1, 2, 4, 7, 2, 4, 3, 2, 2, 2, 2, 3, 3, 5, 8, 8, 6, 7, 3, 1, 7, 6, 5, 2, 5, 5, 4, 1, 7, 4, 0, 2, 2, 4, 3, 7, 8, 6, 5, 3, 3, 8, 7, 2, 2, 3, 3, 5, 5, 8, 4, 1, 2, 1, 2, 5, 5, 4, 8, 5, 5, 7, 7, 8, 5, 7, 6, 2, 7, 1, 7, 3, 3, 7, 7, 6, 7, 7, 3, 4, 5, 6, 6, 4, 3, 5, 5, 2, 2, 2, 1, 3, 7, 8, 2, 6, 3, 2, 5, 2, 5, 5, 3, 6, 1, 6, 3, 6, 3, 2, 2, 4, 6, 2, 5, 3, 7, 4, 2, 1, 7, 8, 3, 7, 1, 4, 8, 3, 4, 2, 3, 4, 6, 7, 1, 5, 8, 8, 1, 8, 1, 5, 7, 2, 6, 1, 7, 7, 5, 2, 5, 6, 6, 6, 8, 6, 8, 1, 2, 7, 7, 2, 6, 5, 1, 6, 3, 8, 7, 6, 3, 9, 5, 6, 3, 5, 7, 2, 4, 2, 4, 5, 1, 3, 6, 2, 3, 5, 7, 8, 5, 2, 1, 3, 3, 7, 2, 1, 4, 5, 7, 4, 7, 5, 8, 5, 1, 7, 8, 2, 4, 2, 3, 4, 2, 4, 3, 2, 3, 3, 3, 3, 2, 1, 7, 5, 8, 8, 3, 5, 1, 6, 7, 1, 2, 6, 4, 6, 5, 5, 2, 9, 8, 1, 3, 2, 2, 2, 2, 2, 1, 2, 3, 8, 5, 5, 7, 7, 3, 1, 1, 3, 4, 6, 5, 5, 4, 3, 2, 5, 2, 3, 2, 8, 4, 6, 7, 3, 4, 1, 5, 1, 6, 2, 7, 3, 5, 7, 3, 1, 7, 6, 3, 5, 2, 2, 5, 2, 2, 5, 7, 3, 4, 9, 6, 1, 8, 2, 3, 3, 7, 8, 2, 1, 2, 5, 8, 7, 5, 1, 4, 1, 3, 2, 7, 5, 4, 1, 3, 4, 6, 6, 5, 5, 3, 7, 3, 8, 8, 3, 7, 7, 6, 2, 1, 7, 8, 5, 3, 6, 1, 5, 5, 5, 3, 7, 5, 1, 5, 3, 3, 5, 6, 1, 7, 1, 7, 8, 6, 5, 3, 1, 6, 3, 3, 4, 3, 6, 2, 4, 5, 4, 3, 6, 4, 7, 1, 5, 3, 2, 1, 3, 1, 2, 1, 4, 7, 8, 5, 2, 2, 1, 2, 2, 4, 3, 2, 1, 1, 4, 6, 7, 5, 3, 5, 5, 7, 4, 8, 3, 4, 7, 4, 5, 3, 8, 9, 5, 1, 4, 3, 4, 6, 5, 2, 2, 5, 5, 5, 4, 1, 5, 2, 7, 7, 1, 2, 2, 5, 1, 8, 3, 1, 1, 2, 5, 8, 1, 7, 2, 7, 1, 3, 6, 3, 5, 5, 7, 3, 1, 3, 7, 7, 6, 5, 8, 3, 1, 1, 1, 1, 3, 2, 1, 4, 7, 5, 5, 5, 1, 3, 4, 1, 3, 2, 3, 3, 1, 7, 4, 2, 6, 4, 3, 2, 3, 8, 3, 3, 6, 4, 7, 3, 7, 4, 2, 3, 7, 6, 3, 6, 6, 5, 1, 2, 2, 7, 5, 5, 5, 5, 6, 2, 1, 7, 4, 3, 5, 7, 2, 3, 4, 6, 6, 5, 4, 1, 6, 5, 1, 1, 5, 5, 3, 7, 0, 1, 1, 2, 6, 3, 1, 7, 7, 1, 7, 7, 6, 1, 1, 2, 8, 3, 4, 7, 9, 2, 3, 3, 8, 3, 5, 7, 4, 1, 6, 1, 5, 5, 3, 8, 7, 1, 2, 3, 1, 1, 5, 1, 7, 6, 4, 4, 3, 2, 9, 7, 7, 3, 4, 4, 6, 3, 4, 8, 8, 4, 3, 5, 1, 8, 7, 5, 3, 2, 7, 6, 3, 2, 7, 2, 8, 3, 7, 3, 2, 8, 5, 2, 5, 3, 5, 5, 2, 8, 4, 5, 1, 1, 5, 2, 5, 2, 3, 3, 3, 4, 1, 3, 5, 7, 2, 8, 5, 5, 6, 3, 5, 2, 0, 3, 2, 3, 3, 1, 7, 2, 3, 5, 7, 1, 3, 7, 3, 1, 5, 8, 3, 3, 5, 2, 6, 7, 4, 2, 3, 8, 3, 9, 2, 2, 8, 2, 4, 3, 8, 3, 7, 1, 2, 8, 5, 6, 2, 1, 7, 4, 2, 1, 3, 1, 7, 7, 3, 5, 2, 6, 2, 5, 7, 5, 3, 1, 6, 2, 1, 3, 3, 3, 4, 5, 2, 7, 5, 9, 6, 3, 3, 2, 1, 2, 3, 3, 1, 7, 5, 1, 4, 6, 7, 1, 7, 1, 8, 6)\n",
      "Creating file 2\n",
      "(5, 2, 5, 7, 6, 4, 3, 6, 1, 3, 7, 3, 6, 1, 9, 2, 6, 4, 1, 6, 1, 2, 6, 7, 3, 2, 1, 1, 1, 0, 2, 7, 3, 7, 6, 6, 1, 6, 4, 1, 2, 1, 5, 1, 3, 7, 5, 8, 1, 8, 2, 5, 5, 3, 5, 7, 0, 8, 6, 6, 4, 7, 1, 2, 8, 2, 4, 6, 2, 1, 7, 4, 2, 8, 2, 7, 3, 2, 7, 7, 4, 1, 3, 7, 6, 8, 2, 2, 2, 8, 1, 1, 4, 3, 5, 4, 8, 3, 3, 6, 7, 2, 5, 5, 2, 1, 1, 7, 1, 1, 3, 1, 3, 1, 4, 2, 2, 7, 2, 8, 8, 7, 5, 3, 1, 1, 6, 1, 1, 5, 6, 5, 6, 3, 7, 2, 5, 2, 5, 4, 8, 4, 3, 3, 6, 5, 5, 5, 5, 3, 6, 5, 4, 2, 4, 4, 8, 1, 7, 5, 2, 1, 7, 1, 5, 3, 2, 5, 3, 5, 1, 5, 4, 7, 1, 4, 3, 7, 7, 8, 0, 5, 6, 6, 3, 3, 2, 6, 7, 3, 2, 2, 2, 5, 1, 2, 1, 3, 6, 2, 8, 2, 3, 2, 8, 8, 2, 1, 1, 7, 8, 1, 4, 6, 2, 7, 5, 7, 3, 5, 8, 8, 6, 3, 1, 7, 2, 6, 9, 1, 2, 5, 7, 3, 4, 7, 7, 5, 8, 7, 7, 7, 3, 6, 5, 5, 8, 1, 1, 7, 6, 2, 7, 1, 8, 1, 4, 2, 2, 3, 3, 7, 2, 3, 4, 1, 4, 6, 3, 3, 3, 2, 5, 1, 7, 2, 5, 7, 7, 4, 6, 2, 7, 7, 1, 3, 8, 1, 1, 2, 5, 0, 8, 5, 5, 5, 2, 5, 3, 3, 4, 1, 4, 5, 8, 5, 5, 3, 7, 7, 3, 6, 5, 7, 4, 7, 5, 1, 5, 1, 7, 8, 5, 7, 7, 6, 1, 4, 1, 8, 2, 4, 9, 7, 3, 0, 7, 2, 5, 6, 9, 3, 5, 2, 1, 4, 9, 4, 3, 1, 6, 8, 3, 8, 5, 2, 3, 7, 6, 2, 8, 3, 2, 7, 3, 4, 3, 6, 4, 1, 8, 8, 4, 6, 5, 3, 6, 2, 8, 3, 5, 2, 5, 8, 5, 5, 3, 3, 5, 4, 2, 2, 2, 2, 1, 1, 7, 8, 8, 8, 2, 8, 2, 9, 3, 8, 5, 7, 4, 9, 5, 7, 6, 1, 3, 5, 1, 6, 7, 7, 3, 7, 8, 4, 2, 1, 7, 4, 1, 7, 3, 6, 4, 7, 2, 1, 7, 7, 5, 6, 6, 3, 3, 7, 4, 4, 7, 7, 9, 3, 1, 4, 8, 8, 5, 8, 3, 5, 2, 4, 3, 4, 7, 6, 5, 8, 8, 1, 3, 1, 2, 5, 2, 6, 4, 3, 3, 2, 3, 4, 1, 4, 6, 4, 7, 2, 8, 2, 5, 1, 1, 2, 7, 8, 1, 7, 8, 7, 3, 5, 5, 5, 8, 7, 8, 6, 4, 7, 4, 2, 2, 5, 2, 7, 8, 8, 3, 4, 6, 8, 5, 2, 4, 5, 1, 1, 8, 3, 4, 8, 5, 7, 8, 3, 5, 7, 8, 5, 7, 4, 9, 7, 3, 1, 3, 2, 8, 1, 7, 3, 1, 7, 8, 6, 7, 2, 7, 0, 1, 3, 1, 2, 7, 1, 4, 3, 8, 6, 7, 3, 5, 5, 7, 6, 5, 3, 4, 6, 7, 8, 4, 4, 3, 1, 5, 5, 2, 7, 8, 2, 1, 1, 3, 3, 7, 1, 8, 7, 2, 7, 2, 6, 8, 4, 3, 1, 4, 7, 7, 3, 1, 3, 5, 3, 3, 1, 1, 6, 3, 6, 3, 1, 6, 3, 7, 8, 9, 7, 8, 4, 1, 2, 1, 6, 4, 4, 6, 1, 8, 9, 6, 1, 9, 5, 7, 2, 7, 2, 4, 8, 9, 2, 8, 5, 4, 2, 1, 8, 6, 7, 8, 4, 3, 5, 5, 3, 3, 6, 4, 2, 3, 4, 3, 1, 4, 2, 7, 6, 3, 9, 5, 7, 6, 8, 3, 1, 2, 7, 7, 7, 3, 9, 7, 7, 5, 4, 5, 1, 4, 2, 2, 5, 4, 1, 5, 1, 2, 8, 2, 2, 5, 3, 4, 2, 2, 3, 7, 3, 8, 2, 8, 4, 5, 2, 3, 8, 1, 7, 4, 8, 1, 5, 7, 0, 3, 1, 2, 4, 7, 1, 2, 6, 5, 4, 4, 3, 7, 3, 7, 5, 5, 5, 6, 4, 2, 3, 8, 3, 6, 6, 3, 6, 3, 7, 5, 3, 3, 2, 2, 2, 7, 3, 1, 7, 5, 3, 8, 5, 3, 7, 5, 4, 7, 7, 1, 8, 3, 2, 8, 5, 1, 2, 5, 5, 3, 3, 7, 4, 6, 2, 7, 8, 8, 5, 2, 3, 2, 1, 5, 7, 2, 1, 5, 3, 4, 5, 8, 5, 5, 8, 3, 1, 1, 3, 2, 2, 7, 4, 4, 2, 3, 2, 8, 3, 6, 4, 6, 3, 4, 3, 2, 6, 3, 2, 1, 4, 5, 7, 4, 3, 2, 1, 7, 4, 0, 4, 2, 1, 7, 2, 6, 8, 2, 7, 5, 6, 2, 5, 6, 1, 3, 5, 4, 2, 8, 5, 5, 8, 7, 2, 2, 1, 1, 1, 3, 7, 7, 7, 4, 6, 5, 3, 5, 1, 5, 1, 4, 3, 8, 5, 4, 3, 8, 0, 4, 5, 7, 7, 1, 6, 3, 7, 1, 7, 1, 7, 1, 3, 6, 7, 4, 7, 2, 8, 2, 3, 1, 7, 4, 1, 5, 3, 1, 1, 6, 4, 5, 7, 2, 1, 1, 7, 2, 6, 5, 5, 7, 1, 2, 5, 1, 1, 3, 3, 2, 7, 1, 6, 3, 3, 2, 6, 5, 2, 1, 8, 2, 3, 1, 5, 2, 1, 5, 3, 5, 8, 6, 3, 5, 5, 1, 2, 7, 2, 8, 3, 5, 7, 3, 2, 3, 4, 2, 5, 2, 1, 6, 0, 5, 3, 7, 2, 4, 4, 5, 4, 8, 6, 2, 5, 3, 3, 3, 2, 2, 1, 6, 4, 8, 4, 7, 8, 7, 1)\n",
      "Creating file 3\n",
      "(1, 6, 2, 4, 2, 7, 1, 2, 2, 3, 2, 6, 5, 7, 7, 6, 4, 7, 6, 2, 3, 2, 3, 5, 5, 5, 1, 6, 5, 7, 5, 2, 6, 3, 1, 7, 5, 7, 3, 7, 2, 7, 7, 2, 7, 3, 7, 7, 1, 7, 7, 7, 3, 1, 3, 2, 4, 4, 5, 3, 2, 2, 8, 4, 5, 8, 5, 8, 2, 3, 4, 3, 4, 7, 4, 1, 1, 5, 5, 5, 7, 3, 2, 7, 1, 8, 8, 1, 5, 2, 2, 6, 2, 7, 2, 1, 7, 6, 8, 7, 1, 6, 5, 3, 6, 5, 6, 1, 6, 8, 7, 4, 1, 1, 2, 7, 4, 8, 7, 1, 1, 8, 2, 2, 5, 8, 7, 4, 3, 7, 5, 5, 4, 6, 2, 1, 7, 7, 4, 2, 7, 5, 5, 5, 7, 3, 4, 5, 3, 3, 5, 4, 7, 6, 2, 5, 5, 8, 7, 5, 8, 5, 6, 3, 8, 4, 1, 8, 1, 6, 1, 3, 8, 5, 7, 1, 5, 4, 3, 5, 8, 3, 8, 2, 7, 3, 1, 2, 3, 2, 1, 3, 3, 3, 1, 8, 5, 3, 5, 3, 2, 5, 5, 6, 3, 7, 5, 4, 6, 2, 2, 2, 5, 3, 4, 7, 2, 5, 3, 3, 1, 7, 2, 1, 1, 5, 1, 5, 1, 1, 8, 6, 7, 5, 1, 1, 2, 1, 7, 7, 2, 2, 5, 1, 2, 4, 1, 4, 3, 6, 5, 1, 5, 7, 5, 3, 4, 8, 4, 4, 6, 7, 2, 5, 4, 1, 2, 3, 1, 5, 7, 3, 1, 2, 8, 2, 4, 8, 4, 2, 9, 3, 7, 7, 6, 8, 8, 4, 2, 1, 5, 6, 8, 3, 1, 8, 7, 7, 6, 5, 2, 5, 8, 1, 1, 2, 1, 7, 2, 1, 3, 6, 4, 6, 7, 2, 3, 6, 1, 2, 1, 5, 1, 7, 2, 7, 2, 2, 8, 5, 3, 7, 6, 3, 5, 6, 9, 8, 6, 7, 1, 5, 3, 5, 8, 1, 3, 7, 5, 2, 3, 2, 2, 5, 3, 8, 5, 3, 7, 8, 6, 2, 7, 3, 6, 5, 1, 1, 8, 7, 7, 6, 7, 8, 8, 8, 6, 2, 4, 6, 3, 4, 3, 7, 1, 2, 6, 3, 8, 1, 6, 5, 1, 4, 8, 7, 4, 6, 7, 8, 3, 5, 7, 1, 2, 8, 2, 2, 7, 8, 8, 5, 8, 1, 4, 7, 1, 2, 1, 7, 8, 5, 7, 8, 8, 2, 3, 4, 8, 1, 7, 6, 2, 2, 1, 2, 3, 1, 6, 7, 2, 7, 3, 6, 7, 7, 5, 3, 2, 5, 5, 1, 6, 6, 1, 6, 3, 8, 8, 5, 8, 4, 3, 1, 1, 3, 8, 1, 7, 1, 3, 9, 2, 8, 8, 6, 2, 3, 4, 4, 6, 2, 7, 5, 2, 1, 3, 3, 2, 5, 7, 7, 1, 7, 5, 5, 4, 7, 1, 1, 5, 9, 7, 5, 5, 3, 7, 7, 3, 5, 3, 6, 7, 1, 2, 5, 8, 4, 2, 8, 3, 2, 4, 7, 1, 5, 4, 2, 7, 3, 1, 7, 2, 1, 2, 7, 7, 1, 3, 3, 1, 3, 2, 2, 4, 2, 8, 5, 3, 3, 2, 3, 1, 3, 2, 5, 2, 1, 7, 0, 5, 4, 8, 7, 8, 3, 8, 2, 3, 3, 1, 7, 8, 5, 6, 3, 2, 1, 3, 8, 3, 3, 7, 4, 8, 5, 5, 5, 2, 3, 6, 5, 2, 2, 0, 7, 3, 5, 6, 5, 5, 5, 7, 5, 5, 6, 6, 7, 5, 3, 2, 1, 2, 4, 2, 1, 2, 5, 7, 5, 6, 7, 7, 6, 7, 6, 7, 8, 7, 7, 3, 6, 6, 4, 3, 7, 1, 1, 1, 7, 7, 7, 1, 4, 3, 7, 7, 4, 8, 7, 7, 1, 5, 5, 6, 6, 1, 5, 8, 3, 1, 9, 8, 3, 5, 6, 6, 1, 8, 2, 1, 7, 0, 5, 3, 9, 1, 2, 3, 3, 1, 5, 7, 1, 2, 5, 2, 0, 5, 7, 7, 5, 7, 5, 3, 8, 3, 1, 5, 3, 6, 2, 8, 5, 3, 3, 6, 7, 4, 2, 5, 7, 5, 1, 8, 3, 1, 3, 3, 5, 8, 2, 5, 7, 8, 1, 2, 7, 2, 6, 2, 0, 2, 4, 6, 5, 6, 1, 5, 2, 7, 4, 4, 2, 3, 8, 3, 8, 3, 5, 3, 3, 6, 5, 1, 8, 8, 5, 3, 1, 5, 8, 7, 2, 2, 7, 3, 7, 5, 8, 2, 3, 3, 3, 8, 2, 3, 6, 4, 4, 2, 4, 5, 6, 1, 8, 8, 4, 5, 5, 1, 3, 3, 6, 5, 3, 7, 7, 1, 8, 4, 3, 2, 1, 2, 1, 8, 2, 8, 3, 1, 3, 3, 3, 5, 5, 2, 2, 1, 1, 5, 7, 3, 2, 8, 1, 6, 2, 4, 2, 4, 1, 7, 2, 5, 8, 2, 2, 6, 3, 7, 5, 2, 8, 3, 3, 5, 8, 1, 5, 3, 6, 3, 7, 1, 7, 5, 7, 5, 1, 7, 1, 5, 3, 1, 1, 1, 5, 2, 1, 6, 7, 1, 5, 5, 1, 3, 1, 5, 8, 8, 7, 3, 6, 7, 1, 5, 1, 7, 1, 4, 3, 7, 4, 5, 3, 2, 8, 5, 1, 3, 5, 7, 2, 8, 1, 1, 2, 8, 8, 4, 1, 4, 6, 6, 3, 5, 6, 3, 3, 3, 8, 2, 5, 3, 6, 4, 1, 3, 5, 5, 5, 4, 7, 5, 6, 1, 1, 8, 6, 6, 3, 6, 5, 3, 8, 2, 1, 7, 7, 4, 2, 5, 8, 2, 3, 3, 5, 8, 5, 3, 3, 2, 6, 5, 5, 7, 3, 5, 1, 5, 3, 4, 3, 5, 2, 7, 6, 8, 1, 7, 4, 7, 6, 1, 3, 7, 5, 8, 1, 2, 5, 6, 2, 7, 1, 8, 8, 5, 8, 1, 5, 6, 1, 4, 5, 7, 2, 0, 3, 3, 7, 6, 3, 3, 5, 2, 7, 7, 1, 3, 3, 6, 8)\n",
      "Creating file 4\n",
      "(1, 5, 1, 7, 2, 1, 3, 1, 7, 3, 7, 5, 1, 8, 2, 6, 5, 2, 7, 4, 5, 2, 4, 7, 3, 1, 4, 2, 8, 4, 3, 7, 7, 7, 6, 2, 7, 2, 7, 3, 8, 2, 1, 6, 6, 3, 6, 7, 5, 2, 6, 4, 6, 3, 7, 3, 5, 4, 3, 1, 8, 8, 6, 5, 2, 3, 1, 7, 5, 5, 3, 5, 2, 2, 3, 8, 5, 1, 8, 2, 7, 2, 2, 7, 2, 1, 5, 1, 1, 1, 3, 6, 6, 7, 8, 2, 8, 7, 2, 6, 1, 4, 7, 3, 2, 5, 4, 5, 4, 7, 5, 3, 3, 2, 5, 2, 6, 7, 7, 6, 5, 8, 3, 5, 7, 1, 3, 1, 2, 1, 2, 1, 5, 3, 3, 6, 7, 4, 5, 3, 2, 8, 2, 5, 6, 7, 3, 1, 2, 2, 1, 1, 7, 1, 6, 4, 2, 7, 1, 7, 5, 6, 2, 2, 1, 3, 5, 7, 1, 8, 3, 4, 2, 5, 1, 2, 7, 6, 2, 5, 5, 5, 3, 8, 2, 6, 2, 7, 0, 1, 1, 7, 7, 2, 7, 6, 7, 3, 3, 8, 5, 3, 7, 4, 1, 6, 6, 5, 2, 3, 8, 3, 1, 4, 3, 8, 2, 1, 7, 4, 2, 4, 2, 2, 7, 4, 7, 6, 1, 6, 3, 7, 7, 2, 5, 6, 5, 1, 5, 3, 5, 3, 3, 8, 3, 5, 5, 1, 5, 9, 2, 4, 8, 2, 2, 5, 2, 1, 3, 1, 7, 0, 5, 1, 2, 3, 7, 6, 5, 2, 1, 6, 1, 5, 3, 2, 7, 6, 3, 7, 1, 2, 2, 7, 6, 8, 3, 1, 1, 1, 7, 3, 4, 1, 5, 2, 8, 8, 2, 3, 4, 4, 3, 9, 7, 3, 1, 1, 1, 3, 6, 3, 3, 3, 5, 7, 1, 3, 4, 5, 1, 1, 3, 5, 1, 2, 8, 7, 3, 3, 5, 5, 1, 5, 2, 3, 1, 8, 6, 8, 3, 7, 7, 8, 2, 7, 9, 1, 5, 3, 5, 1, 4, 2, 2, 3, 2, 7, 7, 9, 1, 6, 5, 2, 2, 5, 8, 9, 1, 4, 4, 2, 8, 2, 8, 8, 7, 2, 6, 4, 6, 8, 1, 2, 3, 6, 7, 8, 5, 1, 1, 3, 4, 8, 1, 1, 8, 2, 3, 2, 3, 7, 2, 3, 5, 8, 3, 2, 5, 2, 6, 2, 2, 3, 8, 9, 7, 7, 1, 7, 5, 5, 8, 6, 6, 2, 1, 1, 1, 4, 6, 7, 2, 1, 2, 1, 8, 8, 1, 3, 3, 2, 2, 8, 8, 1, 3, 7, 5, 6, 2, 4, 1, 7, 1, 1, 2, 3, 2, 4, 5, 1, 1, 6, 2, 2, 7, 7, 2, 5, 0, 6, 5, 4, 1, 1, 3, 2, 6, 9, 3, 3, 5, 7, 4, 2, 6, 1, 7, 5, 5, 3, 6, 3, 4, 7, 7, 3, 1, 7, 1, 2, 5, 7, 5, 1, 3, 1, 1, 7, 7, 5, 4, 3, 0, 5, 5, 1, 3, 6, 3, 7, 7, 2, 3, 5, 3, 3, 2, 6, 7, 5, 5, 7, 1, 8, 5, 6, 5, 6, 2, 1, 2, 8, 2, 1, 6, 7, 2, 6, 3, 3, 3, 8, 2, 5, 1, 5, 7, 0, 1, 1, 7, 7, 2, 3, 7, 1, 5, 8, 4, 3, 6, 2, 6, 4, 5, 7, 4, 8, 7, 7, 2, 3, 1, 3, 4, 3, 1, 5, 1, 1, 3, 5, 7, 7, 7, 6, 5, 2, 3, 4, 5, 2, 3, 1, 1, 4, 8, 1, 2, 3, 3, 0, 5, 1, 2, 8, 0, 7, 1, 2, 7, 4, 4, 3, 4, 7, 6, 3, 1, 2, 1, 5, 6, 7, 1, 8, 1, 5, 3, 5, 1, 5, 5, 2, 6, 5, 6, 5, 6, 5, 8, 1, 4, 2, 5, 4, 7, 6, 5, 7, 5, 1, 7, 8, 6, 1, 1, 7, 6, 6, 2, 3, 8, 6, 2, 8, 5, 5, 1, 9, 6, 6, 3, 5, 8, 5, 1, 4, 8, 2, 3, 1, 2, 1, 5, 6, 7, 3, 2, 1, 4, 7, 3, 7, 4, 3, 6, 2, 3, 8, 2, 3, 6, 2, 4, 2, 5, 7, 4, 4, 4, 1, 7, 2, 5, 7, 2, 3, 6, 4, 3, 5, 8, 5, 7, 7, 7, 4, 5, 9, 1, 7, 6, 6, 2, 2, 1, 6, 2, 7, 5, 3, 1, 2, 1, 4, 6, 5, 6, 5, 7, 2, 2, 5, 6, 6, 2, 3, 1, 7, 7, 3, 1, 7, 2, 4, 1, 5, 8, 7, 2, 6, 4, 3, 1, 4, 2, 6, 1, 2, 4, 4, 6, 3, 4, 1, 7, 5, 3, 0, 8, 7, 7, 5, 1, 1, 4, 2, 6, 7, 7, 2, 4, 2, 5, 7, 2, 1, 6, 3, 8, 6, 3, 3, 7, 3, 1, 7, 7, 9, 6, 5, 5, 7, 2, 1, 8, 3, 5, 5, 2, 3, 2, 1, 4, 1, 7, 3, 5, 5, 7, 2, 1, 3, 8, 2, 3, 2, 1, 3, 3, 1, 4, 3, 8, 7, 6, 1, 8, 2, 2, 6, 6, 6, 5, 4, 5, 2, 7, 8, 2, 2, 7, 8, 5, 7, 3, 1, 5, 7, 3, 7, 7, 3, 7, 7, 9, 8, 5, 8, 8, 7, 1, 6, 3, 2, 7, 5, 3, 2, 3, 6, 8, 3, 4, 0, 2, 2, 7, 5, 2, 3, 1, 1, 9, 4, 7, 7, 5, 1, 7, 1, 3, 3, 8, 6, 6, 6, 5, 3, 8, 8, 1, 7, 4, 6, 3, 3, 1, 3, 5, 5, 8, 6, 8, 2, 3, 3, 6, 4, 2, 1, 7, 1, 4, 4, 1, 7, 1, 3, 1, 2, 7, 6, 5, 3, 1, 2, 5, 5, 7, 2, 2, 3, 5, 5, 0, 3, 4, 5, 5, 5, 7, 8, 8, 4, 8, 5, 8, 8, 4, 1, 1, 3, 8, 6, 5, 2, 4, 8, 2, 5, 3, 5, 1, 7, 5, 4, 1, 5, 7, 8)\n",
      "Creating file 5\n",
      "(7, 7, 1, 4, 5, 3, 5, 1, 1, 1, 6, 6, 1, 5, 5, 7, 3, 6, 1, 7, 1, 6, 8, 7, 1, 8, 1, 2, 6, 1, 2, 4, 3, 1, 1, 3, 1, 7, 6, 1, 4, 3, 1, 3, 5, 5, 4, 6, 5, 2, 5, 7, 3, 7, 3, 3, 7, 1, 8, 1, 8, 3, 1, 3, 9, 2, 2, 5, 6, 5, 3, 5, 8, 4, 6, 6, 1, 8, 1, 1, 8, 7, 2, 8, 5, 5, 5, 2, 3, 4, 2, 7, 1, 5, 1, 5, 1, 4, 4, 2, 3, 6, 2, 2, 6, 4, 8, 7, 6, 8, 1, 5, 1, 8, 2, 2, 4, 4, 5, 5, 6, 5, 5, 1, 7, 6, 6, 7, 3, 6, 4, 8, 7, 1, 1, 0, 3, 2, 4, 7, 8, 4, 8, 8, 7, 9, 4, 2, 8, 5, 4, 7, 3, 6, 4, 1, 5, 4, 1, 2, 1, 7, 2, 5, 3, 8, 3, 4, 8, 5, 2, 1, 2, 7, 7, 8, 5, 5, 8, 8, 2, 4, 2, 3, 7, 5, 2, 5, 7, 5, 2, 6, 1, 2, 7, 6, 3, 6, 2, 8, 7, 1, 5, 4, 5, 8, 2, 5, 3, 5, 6, 4, 5, 1, 3, 6, 8, 5, 4, 7, 4, 5, 8, 1, 7, 3, 8, 5, 6, 1, 2, 1, 3, 1, 2, 7, 8, 7, 5, 2, 5, 1, 5, 7, 2, 4, 7, 5, 2, 6, 0, 7, 1, 7, 4, 9, 6, 3, 3, 5, 1, 8, 3, 3, 1, 1, 2, 3, 5, 2, 4, 3, 1, 2, 8, 1, 4, 1, 4, 7, 5, 3, 9, 2, 5, 4, 8, 5, 2, 3, 3, 4, 3, 7, 6, 9, 2, 1, 2, 8, 3, 8, 3, 3, 8, 7, 6, 1, 7, 7, 3, 2, 6, 3, 4, 9, 6, 7, 1, 1, 2, 1, 3, 8, 2, 7, 2, 3, 5, 2, 5, 1, 6, 2, 8, 6, 6, 3, 2, 6, 6, 1, 5, 5, 3, 2, 2, 6, 3, 5, 2, 7, 6, 2, 6, 6, 7, 1, 2, 3, 2, 4, 0, 8, 7, 3, 2, 7, 1, 2, 3, 8, 2, 3, 1, 2, 8, 6, 0, 8, 5, 7, 9, 7, 3, 1, 7, 7, 2, 2, 1, 5, 1, 5, 1, 7, 7, 5, 5, 8, 5, 7, 3, 5, 8, 7, 7, 7, 4, 4, 5, 3, 7, 7, 4, 1, 5, 0, 8, 5, 6, 3, 2, 2, 6, 5, 7, 7, 8, 1, 2, 7, 3, 3, 7, 1, 7, 3, 1, 3, 1, 5, 3, 2, 4, 1, 2, 1, 7, 8, 5, 5, 7, 7, 1, 1, 4, 8, 3, 8, 1, 3, 3, 7, 3, 5, 1, 2, 5, 7, 5, 1, 5, 2, 7, 0, 5, 2, 2, 5, 6, 8, 1, 2, 6, 5, 7, 7, 2, 6, 4, 1, 4, 3, 6, 4, 4, 7, 5, 6, 3, 1, 6, 7, 3, 1, 2, 8, 1, 3, 6, 7, 7, 8, 3, 5, 8, 3, 3, 7, 4, 7, 8, 5, 3, 4, 7, 5, 0, 0, 4, 2, 1, 2, 7, 5, 6, 1, 2, 1, 1, 7, 2, 6, 7, 4, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "#Specifying the directory name, image size, and no of channels (1 for grayscale)\n",
    "src_filepath_pattern = dirname\n",
    "dest_filepath = dirname+'Assignmenttesth5'\n",
    "n_px = 64\n",
    "n_channels = 1\n",
    "\n",
    "convert_images_to_data_in_h5_file(src_filepath_pattern, dest_filepath, n_px, n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below section was run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install pillow\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir('/content/drive/My Drive/Speech Assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the h5 datafiles\n",
    "def load_dataset(prefix, filelimit):\n",
    "    \n",
    "    lmd_tic = time.time()\n",
    "    \n",
    "    X_full_dataset = []\n",
    "    Y_full_dataset = []\n",
    "    filename_prefix = prefix\n",
    "    \n",
    "    for i in range(1,filelimit):\n",
    "        \n",
    "        filename = filename_prefix + str(i) + \".h5\"\n",
    "        with h5py.File(filename, \"r\") as f:\n",
    "    \n",
    "            X_full_dataset.append(f[\"input_data\"][:])\n",
    "            Y_full_dataset.append(f[\"input_labels\"][:])\n",
    "\n",
    "    lmd_toc = time.time()\n",
    "    print('Time taken to load the data set is', ((lmd_toc-lmd_tic) * 1000), 'ms')\n",
    "    \n",
    "    return X_full_dataset, Y_full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the datasets\n",
    "X_train_dataset, Y_train_dataset = load_dataset('Assignmenttrainh5', 16)\n",
    "X_val_dataset, Y_val_dataset = load_dataset('AssignmentAssignmentvalh5', 3)\n",
    "X_test_dataset, Y_test_dataset = load_dataset('AssignmentAssignmenttesth5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting the dataset from the H5 File\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "#Reshaping and encoding the datasets\n",
    "\n",
    "#Training Set\n",
    "X_train = np.vstack(X_train_dataset)\n",
    "Y_train = np.hstack(Y_train_dataset)\n",
    "\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "enc.fit(Y_train)\n",
    "Y_train = enc.transform(Y_train)\n",
    "Y_train = Y_train.todense()\n",
    "\n",
    "#Validation Set\n",
    "X_val = np.vstack(X_val_dataset)\n",
    "Y_val = np.vstack(Y_val_dataset)\n",
    "\n",
    "Y_val = Y_val.reshape(-1,1)\n",
    "enc.fit(Y_val)\n",
    "Y_val = enc.transform(Y_val)\n",
    "Y_val = Y_val.todense()\n",
    "\n",
    "#Test Set\n",
    "X_test = np.vstack(X_test_dataset)\n",
    "Y_test = np.vstack(Y_test_dataset)\n",
    "\n",
    "Y_test = Y_test.reshape(-1,1)\n",
    "enc.fit(Y_test)\n",
    "Y_test = enc.transform(Y_test)\n",
    "Y_test = Y_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checking types/shapes of vectors we're passing to CNN\n",
    "print(\"Training Set\")\n",
    "print(type(X_train))\n",
    "print(X_train.shape)\n",
    "print(type(Y_train))\n",
    "print(Y_train.shape)\n",
    "print(\"---------------------------\")\n",
    "print(\"Validation Set\")\n",
    "print(type(X_val))\n",
    "print(X_val.shape)\n",
    "print(type(Y_val))\n",
    "print(Y_val.shape)\n",
    "print(\"---------------------------\")\n",
    "print(\"Test Set\")\n",
    "print(type(X_test))\n",
    "print(X_test.shape)\n",
    "print(type(Y_test))\n",
    "print(Y_test.shape)\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import regularizers, optimizers, models\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 64\n",
    "img_width = 64\n",
    "\n",
    "#Define CNN Model\n",
    "input_shape = (img_height, img_width, 1)\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST MODEL TRAINING SECTION\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "#Compile Model\n",
    "model.compile(optimizers.rmsprop(lr=0.0005, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "#Train and Test The Model\n",
    "history_mnist = model.fit(\n",
    "        x = X_train.reshape(X_train.shape[0], img_height, img_width, 1), y = Y_train,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=40,\n",
    "        validation_data = (X_val.reshape(X_val.shape[0], img_height, img_width, 1), Y_val),\n",
    "        validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model to h5 file\n",
    "model.save(\"mnist_model_ver2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-efb5e8c48c07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load model and test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmnist_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist_model_ver2.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "#Load model and test\n",
    "mnist_model = models.load_model('mnist_model_ver2.h5')\n",
    "mnist_model.summary()\n",
    "\n",
    "loss,acc = mnist_model.evaluate(X_test.reshape(X_test.shape[0], img_height, img_width, 1),  Y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot accuracy vs val_accuracy\n",
    "plt.plot(history_mnist.history['accuracy'])\n",
    "plt.plot(history_mnist.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define CNN Model\n",
    "model2 = Sequential()\n",
    "input_shape = (img_height, img_width, 1)\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "nb_epochs = 20\n",
    "model2.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model2.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#Compile Model\n",
    "model2.compile(optimizers.adam(),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "print(model2.summary())\n",
    "\n",
    "#Train and Test The Model\n",
    "cnn_model_history = model2.fit(\n",
    "        x = X_train.reshape(X_train.shape[0], img_height, img_width, 1), y = Y_train,\n",
    "        batch_size = 20,\n",
    "        epochs=30,\n",
    "        verbose = 1,\n",
    "        validation_data = (X_val.reshape(X_val.shape[0], img_height, img_width, 1), Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model to file\n",
    "model2.save(\"cnn_model_with_adam_ver2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model load and evaluation\n",
    "cnn_model = models.load_model('cnn_model_with_adam_ver2.h5')\n",
    "cnn_model.summary()\n",
    "\n",
    "loss,acc = cnn_model.evaluate(X_test.reshape(X_test.shape[0], img_height, img_width, 1),  Y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot accuracy vs val_accuracy\n",
    "\n",
    "plt.plot(cnn_model_history.history['accuracy'])\n",
    "plt.plot(cnn_model_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define CNN Model\n",
    "model3 = Sequential()\n",
    "input_shape = (img_height, img_width, 1)\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "nb_epochs = 20\n",
    "\n",
    "model3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model3.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#Compile Model\n",
    "model3.compile(optimizers.adam(),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "print(model3.summary())\n",
    "\n",
    "#Train and Test The Model\n",
    "history = model3.fit(\n",
    "        x = X_train.reshape(X_train.shape[0], img_height, img_width, 1), y = Y_train,\n",
    "        batch_size = 32,\n",
    "        epochs=100,\n",
    "        verbose = 1,\n",
    "        validation_data = (X_val.reshape(X_val.shape[0], img_height, img_width, 1), Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model to disk\n",
    "model3.save(\"cnn_model_batchnorm_ver2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-9dd320e0c7d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Loading and evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cnn_model_batchnorm_ver2.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "#Loading and evaluation\n",
    "cnn_model = models.load_model(\"cnn_model_batchnorm_ver2.h5\")\n",
    "cnn_model.summary()\n",
    "\n",
    "loss,acc = cnn_model.evaluate(X_test.reshape(X_test.shape[0], img_height, img_width, 1),  Y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Accuracy versus Validation Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
